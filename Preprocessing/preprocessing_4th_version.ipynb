{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load template and defect images\n",
    "template_image = cv2.imread('template.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "defect_image = cv2.imread('image.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Convert images to floating point representation\n",
    "template_image_float = template_image.astype(np.float32)\n",
    "defect_image_float = defect_image.astype(np.float32)\n",
    "\n",
    "# Perform image registration using phase correlation\n",
    "shift = cv2.phaseCorrelate(template_image_float, defect_image_float)\n",
    "\n",
    "# Apply translation shift to defect image\n",
    "rows, cols = defect_image.shape\n",
    "M = np.float32([[1, 0, shift[0][0]], [0, 1, shift[0][1]]])\n",
    "defect_image_aligned = cv2.warpAffine(defect_image, M, (cols, rows))\n",
    "\n",
    "# Compute absolute difference between aligned defect image and template image\n",
    "difference_image = cv2.absdiff(template_image, defect_image_aligned)\n",
    "\n",
    "# Apply threshold to highlight significant differences\n",
    "threshold = 50  # Adjust threshold as needed\n",
    "difference_image[difference_image < threshold] = 0\n",
    "difference_image[difference_image >= threshold] = 255\n",
    "\n",
    "# Visualize the difference image\n",
    "cv2.imshow('Difference Image', difference_image.astype(np.uint8))\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load template and defect images\n",
    "template_image = cv2.imread(r\"E:\\4th sem\\Maths\\project\\pcb\\DeepPCB-master\\PCBData\\group00041\\00041\\00041000_temp.jpg\", cv2.IMREAD_GRAYSCALE)\n",
    "defect_image = cv2.imread(r\"E:\\4th sem\\Maths\\project\\pcb\\DeepPCB-master\\PCBData\\group00041\\00041\\00041000_test.jpg\", cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "\n",
    "# Initialize SIFT detector\n",
    "sift = cv2.SIFT_create()\n",
    "\n",
    "# Detect keypoints and compute descriptors\n",
    "keypoints1, descriptors1 = sift.detectAndCompute(template_image, None)\n",
    "keypoints2, descriptors2 = sift.detectAndCompute(defect_image, None)\n",
    "\n",
    "# Initialize matcher\n",
    "bf = cv2.BFMatcher()\n",
    "matches = bf.knnMatch(descriptors1, descriptors2, k=2)\n",
    "\n",
    "# Apply ratio test\n",
    "good_matches = []\n",
    "for m, n in matches:\n",
    "    if m.distance < 0.75 * n.distance:\n",
    "        good_matches.append(m)\n",
    "\n",
    "# Extract matched keypoints\n",
    "src_pts = np.float32([keypoints1[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "dst_pts = np.float32([keypoints2[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "\n",
    "# Estimate perspective transformation\n",
    "M, _ = cv2.findHomography(dst_pts, src_pts, cv2.RANSAC, 6)\n",
    "\n",
    "# Apply perspective transformation to align defect image\n",
    "rows, cols = defect_image.shape\n",
    "defect_image_aligned = cv2.warpPerspective(defect_image, M, (cols, rows))\n",
    "\n",
    "# Compute absolute difference between aligned defect image and template image\n",
    "difference_image = cv2.absdiff(template_image, defect_image_aligned)\n",
    "\n",
    "# Apply threshold to highlight significant differences\n",
    "threshold = 175  # Adjust threshold as needed\n",
    "difference_image[difference_image < threshold] = 0\n",
    "difference_image[difference_image >= threshold] = 255\n",
    "\n",
    "# Visualize the difference image\n",
    "cv2.imshow('Difference Image', difference_image.astype(np.uint8))\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load template and defect images\n",
    "template_image = cv2.imread(r\"E:\\4th sem\\Maths\\project\\pcb\\DeepPCB-master\\PCBData\\group00041\\00041\\00041000_temp.jpg\", cv2.IMREAD_GRAYSCALE)\n",
    "defect_image = cv2.imread(r\"E:\\4th sem\\Maths\\project\\pcb\\DeepPCB-master\\PCBData\\group00041\\00041\\00041000_test.jpg\", cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "\n",
    "# Initialize SIFT detector\n",
    "sift = cv2.SIFT_create()\n",
    "\n",
    "# Detect keypoints and compute descriptors\n",
    "keypoints1, descriptors1 = sift.detectAndCompute(template_image, None)\n",
    "keypoints2, descriptors2 = sift.detectAndCompute(defect_image, None)\n",
    "\n",
    "# Initialize matcher\n",
    "bf = cv2.BFMatcher()\n",
    "matches = bf.knnMatch(descriptors1, descriptors2, k=2)\n",
    "\n",
    "# Apply ratio test\n",
    "good_matches = []\n",
    "for m, n in matches:\n",
    "    if m.distance < .95 * n.distance:\n",
    "        good_matches.append(m)\n",
    "\n",
    "# Extract matched keypoints\n",
    "src_pts = np.float32([keypoints1[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "dst_pts = np.float32([keypoints2[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "\n",
    "# Estimate perspective transformation using RANSAC\n",
    "M, _ = cv2.findHomography(dst_pts, src_pts, cv2.RANSAC, 6.0)\n",
    "\n",
    "# Apply perspective transformation to align defect image\n",
    "rows, cols = defect_image.shape\n",
    "defect_image_aligned = cv2.warpPerspective(defect_image, M, (cols, rows))\n",
    "\n",
    "# Convert keypoints to NumPy array of floats\n",
    "keypoints2_array = np.array([kp.pt for kp in keypoints2], dtype=np.float32)\n",
    "\n",
    "# Refine alignment using Lucas-Kanade optical flow\n",
    "nextPts, status, _ = cv2.calcOpticalFlowPyrLK(defect_image_aligned, template_image, keypoints2_array, None)\n",
    "\n",
    "# Filter keypoints with valid flow\n",
    "good_new = nextPts[status[:,0] == 1]\n",
    "good_old = keypoints2_array[status[:,0] == 1]\n",
    "\n",
    "# Compute absolute difference between refined aligned defect image and template image\n",
    "difference_image = cv2.absdiff(template_image, defect_image_aligned)\n",
    "\n",
    "# Apply threshold to highlight significant differences\n",
    "threshold = 50  # Adjust threshold as needed\n",
    "difference_image[difference_image < threshold] = 0\n",
    "difference_image[difference_image >= threshold] = 255\n",
    "\n",
    "# Visualize the difference image\n",
    "cv2.imshow('Difference Image', difference_image.astype(np.uint8))\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "cv2.imwrite(\"final.jpg\",difference_image.astype(np.uint8))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Read the image\n",
    "image = cv2.imread('final.jpg')\n",
    "\n",
    "# Convert the image to HSV color space\n",
    "hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "# Define lower and upper bounds for white color\n",
    "lower_white = np.array([0, 0, 200])\n",
    "upper_white = np.array([180, 255, 255])\n",
    "\n",
    "# Threshold the image to get only white colors\n",
    "mask = cv2.inRange(hsv, lower_white, upper_white)\n",
    "\n",
    "# Replace white pixels with red\n",
    "image[mask == 255] = [0, 0, 255]\n",
    "\n",
    "# Show the result\n",
    "cv2.imshow('Result', image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Save the result\n",
    "# cv2.imwrite('result_image.jpg', image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load template and defect images\n",
    "template_image = cv2.imread('final.jpg')\n",
    "defect_image = cv2.imread('edges100.jpg')\n",
    "\n",
    "# Add the two images\n",
    "result_image = cv2.add(template_image, defect_image)\n",
    "\n",
    "# Display the result\n",
    "cv2.imshow('Result Image', result_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not enough good matches for homography estimation.\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.7.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\calib3d\\src\\fundam.cpp:385: error: (-28:Unknown error code -28) The input arrays should have at least 4 corresponding point sets to calculate Homography in function 'cv::findHomography'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 35\u001b[0m\n\u001b[0;32m     32\u001b[0m dst_pts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfloat32([keypoints2[m\u001b[38;5;241m.\u001b[39mtrainIdx]\u001b[38;5;241m.\u001b[39mpt \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m good_matches])\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Estimate perspective transformation using RANSAC\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m M, _ \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfindHomography\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdst_pts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_pts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRANSAC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Apply perspective transformation to align defect image\u001b[39;00m\n\u001b[0;32m     38\u001b[0m rows, cols \u001b[38;5;241m=\u001b[39m defect_image\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.7.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\calib3d\\src\\fundam.cpp:385: error: (-28:Unknown error code -28) The input arrays should have at least 4 corresponding point sets to calculate Homography in function 'cv::findHomography'\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load template and defect images\n",
    "template_image = cv2.imread('final.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "defect_image = cv2.imread('edges100.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Initialize SIFT detector\n",
    "sift = cv2.SIFT_create()\n",
    "\n",
    "# Detect keypoints and compute descriptors\n",
    "keypoints1, descriptors1 = sift.detectAndCompute(template_image, None)\n",
    "keypoints2, descriptors2 = sift.detectAndCompute(defect_image, None)\n",
    "\n",
    "# Initialize matcher\n",
    "bf = cv2.BFMatcher()\n",
    "matches = bf.knnMatch(descriptors1, descriptors2, k=2)\n",
    "\n",
    "# Apply ratio test\n",
    "good_matches = []\n",
    "for m, n in matches:\n",
    "    if m.distance < 0.75 * n.distance:\n",
    "        good_matches.append(m)\n",
    "\n",
    "# Ensure we have enough good matches for homography estimation\n",
    "if len(good_matches) < 4:\n",
    "    print(\"Not enough good matches for homography estimation.\")\n",
    "    exit()\n",
    "\n",
    "# Extract matched keypoints\n",
    "src_pts = np.float32([keypoints1[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "dst_pts = np.float32([keypoints2[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "\n",
    "# Estimate perspective transformation using RANSAC\n",
    "M, _ = cv2.findHomography(dst_pts, src_pts, cv2.RANSAC, 5.0)\n",
    "\n",
    "# Apply perspective transformation to align defect image\n",
    "rows, cols = defect_image.shape\n",
    "defect_image_aligned = cv2.warpPerspective(defect_image, M, (cols, rows))\n",
    "\n",
    "# Add template image and defect image\n",
    "result_image = cv2.add(template_image, defect_image_aligned)\n",
    "\n",
    "# Visualize the result image\n",
    "cv2.imshow('Result Image', result_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (3,) into shape (640,640)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 51\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Create a red mask with the same shape as the difference image\u001b[39;00m\n\u001b[0;32m     50\u001b[0m red_mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros_like(difference_image, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39muint8)\n\u001b[1;32m---> 51\u001b[0m \u001b[43mred_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m)  \u001b[38;5;66;03m# Set intensity to red in RGB color space\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# Apply the red mask to the difference image\u001b[39;00m\n\u001b[0;32m     54\u001b[0m red_difference_image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39maddWeighted(difference_image, \u001b[38;5;241m1\u001b[39m, red_mask, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: could not broadcast input array from shape (3,) into shape (640,640)"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load template and defect images\n",
    "template_image = cv2.imread('template.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "defect_image = cv2.imread('image.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Initialize SIFT detector\n",
    "sift = cv2.SIFT_create()\n",
    "\n",
    "# Detect keypoints and compute descriptors\n",
    "keypoints1, descriptors1 = sift.detectAndCompute(template_image, None)\n",
    "keypoints2, descriptors2 = sift.detectAndCompute(defect_image, None)\n",
    "\n",
    "# Initialize matcher\n",
    "bf = cv2.BFMatcher()\n",
    "matches = bf.knnMatch(descriptors1, descriptors2, k=2)\n",
    "\n",
    "# Apply ratio test\n",
    "good_matches = []\n",
    "for m, n in matches:\n",
    "    if m.distance < 0.75 * n.distance:\n",
    "        good_matches.append(m)\n",
    "\n",
    "# Extract matched keypoints\n",
    "src_pts = np.float32([keypoints1[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "dst_pts = np.float32([keypoints2[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "\n",
    "# Estimate perspective transformation using RANSAC\n",
    "M, _ = cv2.findHomography(dst_pts, src_pts, cv2.RANSAC, 5.0)\n",
    "\n",
    "# Apply perspective transformation to align defect image\n",
    "rows, cols = defect_image.shape\n",
    "defect_image_aligned = cv2.warpPerspective(defect_image, M, (cols, rows))\n",
    "\n",
    "# Convert keypoints to NumPy array of floats\n",
    "keypoints2_array = np.array([kp.pt for kp in keypoints2], dtype=np.float32)\n",
    "\n",
    "# Refine alignment using Lucas-Kanade optical flow\n",
    "nextPts, status, _ = cv2.calcOpticalFlowPyrLK(defect_image_aligned, template_image, keypoints2_array, None)\n",
    "\n",
    "# Filter keypoints with valid flow\n",
    "good_new = nextPts[status[:,0] == 1]\n",
    "good_old = keypoints2_array[status[:,0] == 1]\n",
    "\n",
    "# Compute absolute difference between refined aligned defect image and template image\n",
    "difference_image = cv2.absdiff(template_image, defect_image_aligned)\n",
    "\n",
    "# Create a red mask with the same shape as the difference image\n",
    "red_mask = np.zeros_like(difference_image, dtype=np.uint8)\n",
    "red_mask[:] = (0, 0, 255)  # Set intensity to red in RGB color space\n",
    "\n",
    "# Apply the red mask to the difference image\n",
    "red_difference_image = cv2.addWeighted(difference_image, 1, red_mask, 1, 0)\n",
    "\n",
    "# Visualize the red difference image\n",
    "cv2.imshow('Red Difference Image', red_difference_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Save the red difference image\n",
    "cv2.imwrite(\"red_difference.jpg\", red_difference_image)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
